# E2E Spark Flow
This project handles data from start to finish using Docker, Apache Airflow, Kafka, Spark, Cassandra, and PostgreSQL. 
These tools work together to take in, handle, and keep data.

# System Architecture
![System Architecture](./SparkFlowArchitecture.png)


# Technologies
1. **Apache Airflow**: A platform to programmatically author, schedule, and monitor workflows. It allows you to organize and manage tasks efficiently.
2. **Python**: A popular programming language known for its readability and versatility. It's widely used for web development, data analysis, artificial intelligence, and scientific computing.
3. **Apache Kafka**: A distributed streaming platform. It's used for building real-time data pipelines and streaming apps. It can handle high-throughput data feeds.
4. **Apache Zookeeper**: A centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. It's often used in distributed systems to manage their services.
5. **Apache Spark**: An open-source, distributed processing system used for big data workloads. It provides development APIs in Python, Java, Scala, and R, and an optimized engine that supports general computation graphs.
6. **Cassandra**: A distributed NoSQL database. It's designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.
7. **PostgreSQL**: A powerful, open-source object-relational database system. It uses and extends the SQL language combined with many features that safely store and scale complicated data workloads.
8. **Docker**: A platform for developing, shipping, and running applications. It enables you to separate your applications from your infrastructure so you can deliver software quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, system tools, code, and runtime.
